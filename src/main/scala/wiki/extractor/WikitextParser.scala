package wiki.extractor

import org.sweble.wikitext.parser.nodes.*
import org.sweble.wikitext.parser.utils.NonExpandingParser
import wiki.extractor.language.LanguageLogic
import wiki.extractor.types.{Link, LocatedLink, ParseResult}
import wiki.extractor.util.DBLogging

import scala.jdk.CollectionConverters.IteratorHasAsScala
import scala.reflect.ClassTag
import scala.util.{Failure, Success, Try}

case class IndexedString(text: String, left: Int, right: Int)

class WikitextParser(languageLogic: LanguageLogic) {

  /**
    * Parse wikitext markup for an article using Sweble and retain selected
    * features. See https://github.com/sweble/sweble-wikitext
    *
    * The retained features currently include:
    * - A minimalist plain-text rendition of the page content
    * - The first paragraph of that plain text content
    * - The first sentence of that plain text content
    * - Internal links to other wiki pages
    *
    * @param title  The title of the article for the markup being processed
    * @param markup Wikitext markup
    * @return       Parsed result if Sweble could handle it, otherwise None
    */
  def parseMarkup(title: String, markup: String): Option[ParseResult] = {
    Try {
      parse(title, markup)
    } match {
      case Success(nodes) =>
        Some(processNodes(nodes, markup))
      case Failure(ex) =>
        DBLogging.warn(s"""Could not parse "$title" wikitext markup: ${ex.getClass.getSimpleName}""", both = false)
        None
    }
  }

  /**
    * Recursively extract all nodes of type T into a flattened array.
    *
    * @param nodes Nodes that have been parsed from wikitext
    * @tparam T The node type to extract
    * @return All nodes of type T
    */
  def extractNodes[T <: WtNode: ClassTag](nodes: Array[WtNode]): Array[T] = {
    def collectNodes(node: WtNode): Array[T] = node match {
      case n: T          => Array(n)
      case other: WtNode => other.iterator().asScala.toArray.flatMap(collectNodes)
      case _             => Array()
    }

    nodes.flatMap(collectNodes)
  }

  private[extractor] def parse(title: String, markup: String): Array[WtNode] = {
    parser.parseArticle(markup, title).iterator().asScala.toArray
  }

  private[extractor] def processNodes(input: Array[WtNode], markup: String): ParseResult = {
    // Links as extracted here match the page text exactly.
    // Properly casing the target and stripping sub-page sections happens later.
    val links   = getLocatedLinks(markup)
    val text    = cleanString(nodesToText(input))
    val snippet = languageLogic.getSnippet(text)

    ParseResult(
      snippet = snippet,
      text = text,
      links = links
    )
  }

  /**
    * Get all links from page markup, along with their locations within the
    * page. This was originally a post-processing step for links extracted
    * in a single pass from the node-tree generated by parsing the whole
    * document. However, testing showed that there were cases where the
    * parser grew confused and omitted links when parsing the whole document
    * at once. The same problems do not occur when the bracketed-string
    * fragments are extracted as a separate step and then parsed individually.
    *
    * @param input Markup for a complete Wikipedia page
    * @return      Internal links extracted from the page, along with their
    *              locations within the source text.
    */
  private[extractor] def getLocatedLinks(input: String): Seq[LocatedLink] = {
    WikitextParser
      .getBracketedStrings(input)
      .flatMap { chunk =>
        val markup = s"[[${chunk.text}]]"
        val nodes  = parse(title = "", markup = markup)
        val linksFromChunk = extractNodes[WtInternalLink](nodes).map { internalLink =>
          val link = if (internalLink.hasTitle) {
            Link(target = textualize(internalLink.getTarget).trim, textualize(internalLink.getTitle).trim)
          } else {
            val target = textualize(internalLink.getTarget).trim
            Link(target = target, anchorText = target)
          }

          LocatedLink(link.target, link.anchorText, chunk.left, chunk.right)
        }

        linksFromChunk.find(_.anchorText.trim.nonEmpty)
      }
  }

  private[extractor] def nodesToText(input: Array[WtNode]): String =
    input.map(textualize).mkString

  private[extractor] def textualize(wtNode: WtNode): String = wtNode match {
    case node: WtText                           => node.getContent
    case node: WtImageLink if node.hasTitle     => textualize(node.getTitle)
    case node: WtInternalLink if !node.hasTitle => textualize(node.getTarget)
    case node: WtInternalLink if node.hasTitle  => textualize(node.getTitle)
    case node: WtListItem                       => "\n" + node.iterator().asScala.map(textualize).mkString
    case node: WtTableHeader                    => " : " + node.iterator().asScala.map(textualize).mkString
    case node: WtTableCell                      => " | " + node.iterator().asScala.map(textualize).mkString

    // All of these add noise to the text version of the page. Eliminate
    // template noise, untitled images, XML attributes, and
    // WtTagExtensions (like citations).
    case node: WtImageLink if !node.hasTitle => ""
    case _: WtTemplate                       => ""
    case _: WtXmlAttributes                  => ""
    case _: WtTagExtension                   => ""

    case other: WtNode => other.iterator().asScala.map(textualize).mkString
  }

  private[extractor] def excludeNodes[T <: WtNode: ClassTag](nodes: Array[WtNode]): Array[WtNode] = {
    def collectNodes(node: WtNode): Array[WtNode] = node match {
      case _: T          => Array() // Skip nodes of type T
      case other: WtNode => Array(other)
      case _             => Array()
    }

    nodes.flatMap(collectNodes)
  }

  private[extractor] def cleanString(input: String): String = {
    input
      .replaceAll("[ \t]+", " ")              // Replace multiple spaces or tabs with a single space
      .replaceAll("(?m)^ +| +$", "")          // Remove leading/trailing spaces from each line
      .replaceAll("\n{3,}", "\n\n")           // Replace 3+ newlines with 2 newlines
      .replaceAll("(?m)(\n\\s*){3,}", "\n\n") // Replace 3+ lines containing only whitespace with 2 newlines
      .trim                                   // Remove leading and trailing whitespace from the entire string
  }

  private val parser = new NonExpandingParser(
    true,  // warningsEnabled
    false, // gather round trip data
    false  // autoCorrect
  )
}

object WikitextParser {

  /**
    * Extract all substrings within matching pairs of "[[" and "]]", along
    * with their positions within the input string.
    *
    * @param input Text that may contain zero or more bracket pairs
    * @return      An array of zero or more indexed strings
    */
  def getBracketedStrings(input: String): Array[IndexedString] = {
    // State machine states
    sealed trait State
    case object Outside           extends State
    case object FirstOpenBracket  extends State
    case object Inside            extends State
    case object FirstCloseBracket extends State

    val result       = scala.collection.mutable.ArrayBuffer[IndexedString]()
    var state: State = Outside
    var startPos     = 0
    var i            = 0

    while (i < input.length) {
      val char = input(i)

      state match {
        case Outside =>
          if (char == '[') {
            state = FirstOpenBracket
          }

        case FirstOpenBracket =>
          if (char == '[') {
            // Found "[[" - enter inside state
            state = Inside
            startPos = i + 1 // Position after the opening brackets
          } else {
            // False alarm, go back to outside
            state = Outside
            // Don't increment i to reprocess this character
            i -= 1
          }

        case Inside =>
          if (char == ']') {
            state = FirstCloseBracket
          }

        case FirstCloseBracket =>
          if (char == ']') {
            // Found "]]" - extract the string
            val endPos        = i - 1 // Position before both closing brackets
            val extractedText = input.substring(startPos, endPos + 1)
            result += IndexedString(extractedText, startPos, endPos)
            state = Outside
          } else {
            // False alarm, go back to inside
            state = Inside
            // Don't increment i to reprocess this character
            i -= 1
          }
      }

      i += 1
    }

    result.toArray
  }
}
